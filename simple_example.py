"""
RAGã‚·ã‚¹ãƒ†ãƒ ã®è»½é‡ç‰ˆãƒ‡ãƒ¢ï¼ˆGeminiæœ€æ–°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
"""

import os
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

def simple_rag_example():
    """
    è»½é‡ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("ğŸ“š RAGã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œä¾‹ï¼ˆæœ€æ–°ç‰ˆï¼‰")
    print("=" * 50)
    
    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key or api_key == 'your-gemini-api-key-here':
        print("âŒ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ğŸ“ .envãƒ•ã‚¡ã‚¤ãƒ«ã§GOOGLE_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã®ä½œæˆ
    sample_documents = [
        "Python ã¯æ±ç”¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚Webã‚¢ãƒ—ãƒªã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€AIé–‹ç™ºã«åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹AIæŠ€è¡“ã§ã™ã€‚",
        "RAGã¯æ¤œç´¢æ‹¡å¼µç”Ÿæˆã¨å‘¼ã°ã‚Œã€æ¤œç´¢ã—ãŸæƒ…å ±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šæ­£ç¢ºãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹æ‰‹æ³•ã§ã™ã€‚",
        "ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ã§ä¿å­˜ã—ã€é¡ä¼¼æ¤œç´¢ã‚’é«˜é€Ÿã«è¡Œã†ä»•çµ„ã¿ã§ã™ã€‚",
        "LangChainã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚"
    ]
    
    # Documentå½¢å¼ã«å¤‰æ›
    documents = [Document(page_content=content, metadata={"source": f"doc_{i}"}) 
                for i, content in enumerate(sample_documents)]
    
    print(f"âœ… {len(documents)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’æº–å‚™ã—ã¾ã—ãŸ")
    
    # æ–‡æ›¸åˆ†å‰²å™¨ã®è¨­å®š
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len,
    )
    
    # æ–‡æ›¸ã‚’åˆ†å‰²ï¼ˆä»Šå›ã¯ã‚µãƒ³ãƒ—ãƒ«ãŒçŸ­ã„ã®ã§ãã®ã¾ã¾ä½¿ç”¨ï¼‰
    chunks = text_splitter.split_documents(documents)
    print(f"âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
    
    try:
        # GoogleåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        print("ğŸ”„ GoogleåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=api_key
        )
        print("âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†")
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
        print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        vector_store = FAISS.from_documents(
            documents=chunks,
            embedding=embeddings
        )
        print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå®Œäº†")
        
        # è»½é‡ãªFlashãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚’å›é¿ï¼‰
        model_names = [
            "models/gemini-1.5-flash-8b",      # æœ€è»½é‡ãƒ¢ãƒ‡ãƒ«
            "models/gemini-2.0-flash-lite",    # è»½é‡ãƒ¢ãƒ‡ãƒ«
            "models/gemini-1.5-flash",         # æ¨™æº–Flashãƒ¢ãƒ‡ãƒ«
            "models/gemini-2.0-flash",         # æ–°ã—ã„Flashãƒ¢ãƒ‡ãƒ«
            "models/gemini-1.5-flash-latest"   # æœ€æ–°Flashãƒ¢ãƒ‡ãƒ«
        ]
        
        llm = None
        for model_name in model_names:
            try:
                print(f"ğŸ”„ {model_name} ã‚’è©¦è¡Œä¸­...")
                llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=0.1,  # ã‚ˆã‚Šä½ã„æ¸©åº¦ã§å®‰å®šæ€§ã‚’é‡è¦–
                    google_api_key=api_key,
                    max_tokens=500,   # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã‚’è¿½åŠ 
                    request_timeout=30  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
                )
                print(f"âœ… {model_name} ã§åˆæœŸåŒ–å®Œäº†ï¼ˆãƒ†ã‚¹ãƒˆå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                break
            except Exception as model_error:
                error_msg = str(model_error)
                if "quota" in error_msg.lower() or "429" in error_msg:
                    print(f"âŒ {model_name}: ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¾ã—ãŸ")
                    import time
                    print("â±ï¸ 10ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ...")
                    time.sleep(10)
                else:
                    print(f"âŒ {model_name}: {error_msg}")
                continue
        
        if llm is None:
            print("âŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¦ã„ã¾ã™")
            print("ğŸ’¡ å¯¾å‡¦æ³•:")
            print("  1. æ•°æ™‚é–“å¾…ã£ã¦ã‹ã‚‰å†å®Ÿè¡Œ")
            print("  2. Google AI Studioã§æ–°ã—ã„APIã‚­ãƒ¼ã‚’ä½œæˆ")
            print("  3. æœ‰æ–™ãƒ—ãƒ©ãƒ³ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰")
            return
        
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt_template = """
ä»¥ä¸‹ã®æ–‡è„ˆæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

æ–‡è„ˆ: {context}

è³ªå•: {question}

å›ç­”ã¯æ–‡è„ˆæƒ…å ±ã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚
"""
    
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    
    # RetrievalQAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    print("\nğŸ’¬ è³ªå•å¿œç­”ãƒ‡ãƒ¢")
    print("=" * 50)
    
    # ãƒ‡ãƒ¢è³ªå•
    questions = [
        "Pythonã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„"
    ]
    
    for question in questions:
        print(f"\nâ“ è³ªå•: {question}")
        
        # é–¢é€£æ–‡æ›¸ã®æ¤œç´¢
        relevant_docs = vector_store.similarity_search(question, k=2)
        print(f"ğŸ” é–¢é€£æ–‡æ›¸ã‚’{len(relevant_docs)}ä»¶æ¤œç´¢ã—ã¾ã—ãŸ")
        print("ğŸ“– æ¤œç´¢ã•ã‚ŒãŸæ–‡è„ˆ:")
        for doc in relevant_docs:
            print(f"  {doc.page_content}")
        
        try:
            # è³ªå•å¿œç­”
            print("ğŸ¤– GeminiãŒå›ç­”ã‚’ç”Ÿæˆä¸­...")
            response = qa_chain.invoke({"query": question})
            print(f"ğŸ’¡ å›ç­”: {response['result']}")
            
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            print("ï¿½ ç›´æ¥çš„ãªè³ªå•å¿œç­”ã‚’è©¦è¡Œä¸­...")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥LLMã§è³ªå•
            context = "\n".join([doc.page_content for doc in relevant_docs])
            direct_prompt = f"""
ä»¥ä¸‹ã®æ–‡è„ˆæƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆæƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”ã¯æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
"""
            try:
                direct_response = llm.invoke(direct_prompt)
                print(f"ğŸ’¡ ç›´æ¥å›ç­”: {direct_response.content}")
            except Exception as direct_error:
                print(f"âŒ ç›´æ¥å›ç­”ã‚‚å¤±æ•—: {direct_error}")
                print("ï¿½ğŸ“ æ¤œç´¢çµæœã®ã¿è¡¨ç¤º:")
                for doc in relevant_docs:
                    print(f"  â€¢ {doc.page_content}")
        
        print("-" * 30)
    
    print("\nâœ¨ ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
    print("ğŸ¯ RAGã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã—ã¾ã—ãŸ")

if __name__ == "__main__":
    simple_rag_example()